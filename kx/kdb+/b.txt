TRILLION ROW BENCHMARKS

dataset: nyse TAQ 5000 days. 1.1Trillion quotes and 65Billion trades. raw text 100TB.
machine: 16 core 256GB (in all cases: date partition, sym index. all queries in RAM.)
product: q4, q3, big3rdbms(+accelerated), postgres, cloudera, mongodb, columnar, spark/shark, ..
queries: simple aggregations on top 100 symbols (1/4th of data) plus a regnms (price<current bid).

Q1: select last bid  by sym         from quote where date=d,sym in S
Q2: select max price by sym,ex      from trade where date=d,sym in S
Q3: select avg size  by sym,time.hh from trade where date=d,sym in S
Q4: select time,price,bid from aj[`time;select from trade where date=d,sym=`CSCO;select from quote where date=d,sym=`CSCO]where price<bid

Query times are milliseconds, e.g. on Query3 q3 is 200 times faster than spark/shark.

Small day      (2003.09.10  35Million quotes.  5Million trades.)
               Q1      Q2      Q3      Q4 overhead(us) RAM(GB)    ETL      DSK
q4              1       7      12       4        1        .06      20      1GB
q3             18      12      42       9       10        .09      70      1GB 
cloudera     4800    1190    1000     DNF   100000       4.0       22    .25GB
big3rdbms    6400    2200    3100     DNF      300       5.0       60      2GB 
big3accel    4200    1600    2300     DNF      300       3.4       20      1GB  
postgres     6800    5200    4800     DNF      100        .1      200      6GB
mongodb      8900    1700    5800     DNF      100       9.0      922     10GB 
spark/shark 34000    7400    8400     DNF      100      50.0      156    2.4GB                              
columnar      500     140     150    8900     4000       2.1       52     .5GB

SCALE UP       (2013.04.03 610Million quotes. 29Million trades.)
q4              1      35      40       8
q3            290      33     130      29
cloudera    45000    2250    1880     DNF

SCALE OUT(q4)  (1600 days: 980Billion quotes. 46Billion trades.)
   1 days:      1      35      40       8    /       640Million rows
  16 days:     10      60      70      17    /        10Billion rows
  32 days:     17      83     100      20    /        20Billion rows
1600 days:    750    3500    5000     900    /     1,000Billion rows

all database products should index by sym so the number of rows scanned should be about 25Billion.
most database products thrash on Q1 and cannot do Q4 (asof join) on big tables. DNF is did not finish.
overhead is milliseconds for fastest query, e.g. q3 can do 100,000 different queries per second per cpu.
RAM is memory usage for queries. in all cases all query data is cached in RAM (no disk access).
ETL is seconds to load 2003.09.10 (34M quotes and 5M trades) and index on date and sym.

Queries in the other languages:

q4
 {select last bid             from quote[d]x}'S
 {select max price by ex      from trade[d]x}'S
 {select avg size  by `h$time from trade[d]x}'S
 select time,price,bid from(trade[d]`CSCO;quote[d]`CSCO)where price<bid

sql
 select sym,bid from (select sym,bid,row_number()over(partition by sym order by time desc)as row from quote left semi join S on quote.sym=S.sym where date=d)q where row=1
 select sym,ex,max(price) from trade where date=d and sym in(select sym from S)group by sym,ex
 select sym,hour(time),avg(size) from trade left semi join S on trade.sym=S.sym where date=d group by sym,hour(time)
 select * from (select time,price,sym from trade where date=d and sym=`CSCO) t 
  left outer join (select time,bid,sym from quote where date=d and sym=`CSCO) q
  on q.time=(select max(time) from q where time<=t.time and sym=`CSCO) where price<bid

mongo
S.map(function(x){return db.quote.aggregate([{"$match":{date:d,sym:x}},{"$sort":{time:1}},{"$group":{"_id":{sym:"$sym"},bid:{"$last":"$bid"}}}]).toArray()})
S.map(function(x){return db.trade.aggregate([{"$match":{date:d,sym:x}},{"$group":{"_id":{sym:"$sym",ex:"$ex"},price:{"$max":"$price"}}}]).toArray()})
S.map(function(x){return db.trade.aggregate([{"$match":{date:d,sym:x}},{"$group":{"_id":{sym:"$sym",hour:{"$substr":["$time",0,2]}},size:{"$avg":"$size"}}}]).toArray()})
?

generating S: top 100 syms
q4:    S:100#>count'trade d
q3:    100#idesc exec count i by sym from trade where date=d
sql:   create table S(sym char(4));insert into S select sym from trade where date=d group by sym order by count(*) desc fetch first 100 rows only
mongo: db.trade.aggregate([{$group:{_id:{s:"$s"},n:{"$sum":1}}},{$sort:{n:-1}},{$limit:100},{$project:{_id:0,s:"$_id.s"}}]).toArray().map(function(x){return x.s;})

scalability: q scale's well. the others may too but watch out for RAM usage.
ram/ssd/dsk: q uses ram, ssd and dsk gently. column store and simplicity helps.



TRILLION ROW BENCHMARKS

k&q 10 to 100 times faster than the colstore(vertical, big3accel, hadoop/impala, ..)
k&q 100 to 1000 times faster than rowstore(postgres, big3rdbms, mongodb, spark, ..)
k&q scale out best: lowest overhead. fastest messaging. lowest RAM usage. (hadoop overhead is ridiculous)
other products struggle with order. Q1 and Q4. (order is anathema to rdbms).

dataset: nyse TAQ 5000 days. 1.1Trillion quotes and 65Billion trades. raw text 100TB.
machine: 16 core 256GB (in all cases: date partition, sym index. all queries in RAM.)
product: k5, q3, big3rdbms, big3accel, postgres, hadoop/impala/parquet, mongodb, columnar(postgres+cstore), spark/shark, ..
queries: 3 aggregations on top 100 symbols (1/4th of data) plus a regnms (price<current bid).

Q1: select last bid  by sym         from quote where date=d,sym in S
Q2: select max price by sym,ex      from trade where date=d,sym in S
Q3: select avg size  by sym,time.hh from trade where date=d,sym in S
Q4: select time,price,bid from aj[`time;select from trade where date=d,sym=`CSCO;select from quote where date=d,sym=`CSCO]where price<bid

Query times are milliseconds, e.g. q3 Query3 is 200 times faster than spark/shark.

Small(40Million rows)
                Q1      Q2      Q3      Q4   RAM(GB)    ETL   DSK(GB) OVERHEAD
k5               1      16      10       2      .1       20      1           1
q3              17      12      36       6      .2       70      1          10
vertical       500     140     150    8900     2.1       52       .5      4000
hadoop/impala 4800    1190    1000     DNF     4.0       22       .3    100000
big3accel     4200    1600    2300     DNF     3.4       20      1         300
postgres      7100    1500    1900     DNF     1.5      200      4         100
big3rdbms     6400    2200    3100     DNF     5.0       60      2         300
mongodb       8900    1700    5800     DNF     9.0      922     10         100
spark/shark  34000    7400    8400     DNF    50.0      156      2.4       100                          

SCALE UP(1 big day:     640Million rows)
k5               1      35      40       8
q3             290      33     130      29
impala       45000    2250    1880     DNF

SCALE OUT(16 big days:   10Billion rows)
k5              10      60      70      17
q3             820      70     180      70
impala    10000000   18000   27000     DNF

SCALE OUT with k5
   1 days:      1      35      40       8    /        .6Billion rows
  16 days:     10      60      70      17    /        10Billion rows
  32 days:     17      83     100      20    /        20Billion rows
1600 days:    750    3500    5000    1000    /     1,000Billion rows

all database products index by sym so the number of rows scanned should be about 25Billion.(but most fail on Q1)
most database products thrash on Q1 and cannot do Q4 (asof join) on big tables. DNF is did not finish.
overhead is microseconds for fastest query, e.g. q3 can do 100,000 different queries per second per cpu.
RAM is memory usage for queries. in all cases all query data is cached in RAM (no disk access).
ETL is seconds to load 2003.09.10 (34M quotes and 5M trades) and index on date and sym.

Queries in the other languages:

k5
 {select last bid             from quote[d]x}'S
 {select max price by ex      from trade[d]x}'S
 {select avg size  by `h$time from trade[d]x}'S
 select time,price,bid from(trade[d]`CSCO;quote[d]`CSCO)where price<bid

sql
 select sym,bid from (select sym,bid,row_number()over(partition by sym order by time desc)as row from quote left semi join S on quote.sym=S.sym where date=d)q where row=1;
 select sym,ex,max(price) from trade where date=d and sym in(select sym from S)group by sym,ex;
 select sym,hour(time),avg(size) from trade left semi join S on trade.sym=S.sym where date=d group by sym,hour(time);
 select * from (select time,price,sym from trade where date=d and sym=`CSCO) t
  left outer join (select time,bid,sym from quote where date=d and sym=`CSCO) q
  on q.time=(select max(time) from q where time<=t.time and sym=`CSCO) where price<bid;

mongo
S.map(function(x){return db.quote.aggregate([{"$match":{date:d,sym:x}},{"$sort":{time:1}},{"$group":{"_id":{sym:"$sym"},bid:{"$last":"$bid"}}}]).toArray()})
S.map(function(x){return db.trade.aggregate([{"$match":{date:d,sym:x}},{"$group":{"_id":{sym:"$sym",ex:"$ex"},price:{"$max":"$price"}}}]).toArray()})
S.map(function(x){return db.trade.aggregate([{"$match":{date:d,sym:x}},{"$group":{"_id":{sym:"$sym",hour:{"$substr":["$time",0,2]}},size:{"$avg":"$size"}}}]).toArray()})
?

generating S: top 100 syms
k5:    S:100#>count'trade d
q3:    S:100#first flip idesc select count i by sym from trade where date=d
sql:   create table S(sym char(4));insert into S select sym from trade where date=d group by sym order by count(*) desc fetch first 100 rows only
mongo: db.trade.aggregate([{$group:{_id:{s:"$s"},n:{"$sum":1}}},{$sort:{n:-1}},{$limit:100},{$project:{_id:0,s:"$_id.s"}}]).toArray().map(function(x){return x.s;})

scalability: k&q scale well. the others may too but watch out for RAM usage.
ram/ssd/dsk: k&q use ram, ssd and dsk gently. column store and simplicity helps.

